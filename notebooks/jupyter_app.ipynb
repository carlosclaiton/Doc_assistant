{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook if parts of the App for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "with open(\"../CONFIG_LIST.json\", \"r\") as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = config[\"openai_api_key\"]\n",
    "model = config[\"model\"]\n",
    "\n",
    "openai.organization = config[\"openai_organization\"]\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "documents = SimpleDirectoryReader('../documentS').load_data() # reads whole drectory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using OpenAI API directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index exist\n"
     ]
    }
   ],
   "source": [
    "from llama_index import LLMPredictor, PromptHelper, ServiceContext\n",
    "from llama_index import GPTVectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "def indexer(documents, model ):\n",
    "    \n",
    "    if os.path.exists('../storage'):\n",
    "        print('index exist')\n",
    "        \n",
    "        # rebuild storage context\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=\"../storage\")\n",
    "        # load index\n",
    "        index = load_index_from_storage(storage_context)\n",
    "\n",
    "    \n",
    "    else:\n",
    "        print('not exist, creating index ... ')\n",
    "\n",
    "        documents = documents\n",
    "        \n",
    "        llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=model))\n",
    "\n",
    "        max_input_size = 4096\n",
    "        num_output = 256\n",
    "        max_chunk_overlap = 0.1\n",
    "        chunk_size_limit = 600\n",
    "        prompt_helper = PromptHelper(max_input_size, num_output,max_chunk_overlap,chunk_size_limit=chunk_size_limit)\n",
    "        service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "        # index content in the folder documents\n",
    "        index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context) \n",
    "        # Save your index to a directory called storage\n",
    "        index.storage_context.persist(persist_dir=\"../storage\")\n",
    "    \n",
    "    return(index)\n",
    "\n",
    "index = indexer(documents=documents, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book 'Exploring the Depths of Human Existence' is a project that has taken over 12 years to complete. It is a collection of conversations on identity, society, and progress, written by Dr. Carlos Kuhn. The author acknowledges that we do not have all the answers and encourages curiosity and the pursuit of knowledge. The book aims to captivate readers' interest, spark excitement, and inspire thoughtful contemplation. It is a culmination of insights and data, crafted and refined over time, and is intended to serve as a catalyst for intellectual exploration.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"Can you write a summary of the book 'Exploring the Depths of Human Existence' \"\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me : What is the autor name for the book in the index?\n",
      "Agent : The author name for the book in the index is Dr Carlos Kuhn.\n",
      " ---------- Agent: chat is closed -------\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Define tools\n",
    "tools = [\n",
    "    Tool(\n",
    "       name = \"LlamaIndex\",\n",
    "        func=lambda q: str(index.as_query_engine().query(q)),\n",
    "        description=\"useful for when you want to answer questions about the author. The input to this tool should be a complete english sentence.\",\n",
    "        return_direct=True\n",
    "    ),\n",
    "]\n",
    "#Initialize conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory( memory_key='chat_history', k=5, return_messages=True )\n",
    "# Initialize agent with conversational memory\n",
    "agent_executor = initialize_agent(tools, llm=ChatOpenAI(temperature=0.7, model_name=model), agent=\"conversational-react-description\", memory=conversational_memory)\n",
    "\n",
    "# if I use my index\n",
    "while True:\n",
    "    prompt = input(\"type prompt\")\n",
    "    if prompt == 'thanks':\n",
    "        print(f' ---------- Agent: chat is closed -------')\n",
    "        break\n",
    "    else:\n",
    "        response = agent_executor.run(input=prompt)\n",
    "        print(f'me : {prompt}')\n",
    "        print(f'Agent : {response}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do to improve the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory has not changed.\n"
     ]
    }
   ],
   "source": [
    "# Todo: Need to modify, to generate new index if document is added to the folder documents\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "def get_directory_hash(directory):\n",
    "    file_hash_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "                file_hash_list.append((file_path, file_hash))\n",
    "    return file_hash_list\n",
    "\n",
    "def has_directory_changed(previous_state, current_state):\n",
    "    return previous_state != current_state\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = \"documents\"\n",
    "\n",
    "    # Record the current state\n",
    "    current_state = get_directory_hash(directory_path)\n",
    "\n",
    "    # Compare with the previous state (you might want to load the previous state from a file)\n",
    "    if has_directory_changed(previous_state, current_state):\n",
    "        print(\"Directory has changed!\")\n",
    "        # Update the previous state with the current state for the next check\n",
    "        previous_state = current_state\n",
    "    else:\n",
    "        print(\"Directory has not changed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_state = get_directory_hash(\"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('AIbot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c5a3cddc1d3006194dcb49c239b97b7818ce13a2d82a05f125c47421e5d8bdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
