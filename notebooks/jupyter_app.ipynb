{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook if parts of the App for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "with open(\"../CONFIG_LIST.json\", \"r\") as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = config[\"openai_api_key\"]\n",
    "model = config[\"model\"]\n",
    "\n",
    "openai.organization = config[\"openai_organization\"]\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# documents = SimpleDirectoryReader('../documents').load_data() # reads whole drectory.\n",
    "documents = SimpleDirectoryReader('../MyBook').load_data() # reads whole drectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using OpenAI API directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not exist, creating index ... \n"
     ]
    }
   ],
   "source": [
    "from llama_index import LLMPredictor, PromptHelper, ServiceContext\n",
    "from llama_index import GPTVectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "def indexer(documents, model ):\n",
    "    \n",
    "    try:\n",
    "            # rebuild storage context\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=\"../storage\")\n",
    "            # load index\n",
    "            index = load_index_from_storage(storage_context)\n",
    "    except:\n",
    "        \n",
    "        print('not exist, creating index ... ')\n",
    "\n",
    "        documents = documents\n",
    "        \n",
    "        llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=model))\n",
    "        # llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"text-davinci-003\"))\n",
    "\n",
    "        max_input_size = 4096\n",
    "        num_output = 256\n",
    "        max_chunk_overlap = 0.1\n",
    "        chunk_size_limit = 600\n",
    "        prompt_helper = PromptHelper(max_input_size, num_output,max_chunk_overlap,chunk_size_limit=chunk_size_limit)\n",
    "        service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "        # index content in the folder documents\n",
    "        index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context) \n",
    "        # Save your index to a directory called storage\n",
    "        index.storage_context.persist()\n",
    "        index.storage_context.persist(persist_dir=\"../storage\")\n",
    "    \n",
    "    return(index)\n",
    "\n",
    "index = indexer(documents=documents, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book 'Exploring the Depths of Human Existence' by Dr. Carlos Kuhn is a comprehensive exploration of topics related to identity, society, and progress. It's a work of dedication that took the author over 12 years to complete, originating from a humble beginning and gradually amassing insights and data. The author's goal is to inspire readers to engage in thoughtful contemplation and to catalyze their own intellectual exploration. The book acknowledges the fact that we do not have all the answers, a realization that fuels curiosity and drives the continuous pursuit of knowledge and personal growth.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"Can you write a summary of the book 'Exploring the Depths of Human Existence' \"\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me : what is the book title?\n",
      "Agent : I'm sorry, I can't provide the title of a book without more context. Can you please provide more details or clarify your question?\n",
      "me : what is the book title in the index I have feed you with\n",
      "Agent : The title of the book in the index provided is \"Exploring the Depths of Human Existence\".\n",
      " ---------- Agent: chat is closed -------\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Define tools\n",
    "tools = [\n",
    "    Tool(\n",
    "       name = \"LlamaIndex\",\n",
    "        func=lambda q: str(index.as_query_engine().query(q)),\n",
    "        description=\"useful for when you want to answer questions about the author. The input to this tool should be a complete english sentence.\",\n",
    "        return_direct=True\n",
    "    ),\n",
    "]\n",
    "#Initialize conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory( memory_key='chat_history', k=5, return_messages=True )\n",
    "# Initialize agent with conversational memory\n",
    "agent_executor = initialize_agent(tools, llm=ChatOpenAI(temperature=0.7, model_name=model), agent=\"conversational-react-description\", memory=conversational_memory)\n",
    "\n",
    "# if I use my index\n",
    "while True:\n",
    "    prompt = input(\"type prompt\")\n",
    "    if prompt == 'thanks':\n",
    "        print(f' ---------- Agent: chat is closed -------')\n",
    "        break\n",
    "    else:\n",
    "        response = agent_executor.run(input=prompt)\n",
    "        print(f'me : {prompt}')\n",
    "        print(f'Agent : {response}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do to improve the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory has not changed.\n"
     ]
    }
   ],
   "source": [
    "# Todo: Need to modify, to generate new index if document is added to the folder documents\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "def get_directory_hash(directory):\n",
    "    file_hash_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "                file_hash_list.append((file_path, file_hash))\n",
    "    return file_hash_list\n",
    "\n",
    "def has_directory_changed(previous_state, current_state):\n",
    "    return previous_state != current_state\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = \"documents\"\n",
    "\n",
    "    # Record the current state\n",
    "    current_state = get_directory_hash(directory_path)\n",
    "\n",
    "    # Compare with the previous state (you might want to load the previous state from a file)\n",
    "    if has_directory_changed(previous_state, current_state):\n",
    "        print(\"Directory has changed!\")\n",
    "        # Update the previous state with the current state for the next check\n",
    "        previous_state = current_state\n",
    "    else:\n",
    "        print(\"Directory has not changed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_state = get_directory_hash(\"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('AIbot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c5a3cddc1d3006194dcb49c239b97b7818ce13a2d82a05f125c47421e5d8bdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
